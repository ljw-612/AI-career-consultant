{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_j = ['data', 'ai', 'data', 'data science', 'data scraping', 'data', 'database', 'data scraping', 'software', 'data analytics', 'python', 'database querying', 'mysql']\n",
    "list_r = ['python', 'sql', 'data visualization', 'data', 'python', 'r', 'data science', 'machine']\n",
    "\n",
    "list_r =  ['ma', 's', 'data visualization', 'analytics', 'databricks', 'data management', 'git', 'machine learn', 'matlab', 'java', 'scikit', 'python', 'gcp', 'r', 'sql', 'javascript', 'dart', 'sas', 'or', 'ge', 'data science', 'database management', 'n', 'p', 'neural', 'machine', 'pyspark', 'sea', 'excel', 'vision']\n",
    "list_j =  ['python', 'data', 'database', 'ai', 'data scraping', 'software', 'data analytics', 'database querying', 'data science']\n",
    "\n",
    "list_j = ['data scraping', 'ai', 'data science', 'database', 'data', 'data analytics', 'software', 'database querying', 'python']\n",
    "list_r = ['dart', 'sas', 'ge', 'analytics', 'neural', 'excel', 'sea', 'ma', 'or', 'data management', 'n', 'scikit', 'sql', 'git', 'database management', 'machine', 's', 'data visualization', 'machine learn', 'vision', 'r', 'gcp', 'data science', 'javascript', 'matlab', 'pyspark', 'java', 'databricks', 'p', 'python']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(client, chunks):\n",
    "    '''\n",
    "    This function creates embeddings for the chunks of text using the OpenAI API.\n",
    "    '''\n",
    "    \n",
    "    def _make_embedding(client, chunk, model=\"text-embedding-3-small\"):\n",
    "        chunk = chunk.replace(\"\\n\", \" \")\n",
    "        return client.embeddings.create(input = [chunk], model=model).data[0].embedding\n",
    "    \n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embedding = _make_embedding(client, chunk)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "def make_meta_embedding(embeddings, skills):\n",
    "    meta_embedding = {}\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        meta_embedding[skills[i]] = embedding\n",
    "    return meta_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "    \n",
    "openai_key = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embedding_j = make_meta_embedding(make_embeddings(client, list_j), list_j)\n",
    "meta_embedding_r = make_meta_embedding(make_embeddings(client, list_r), list_r)\n",
    "\n",
    "meta_embedding_j_copy = meta_embedding_j.copy()\n",
    "# Compute the cosine similarity between the meta embeddings of the two lists\n",
    "# missing_skills = []\n",
    "\n",
    "\n",
    "# Assuming meta_embedding_j and meta_embedding_r are dictionaries with skills as keys and embeddings as values\n",
    "# similarities = {}\n",
    "\n",
    "for skill_j, embedding_j in meta_embedding_j.items():\n",
    "    for skill_r, embedding_r in meta_embedding_r.items():\n",
    "        # Compute the cosine similarity between the two embeddings\n",
    "        similarity = cosine_similarity([embedding_j], [embedding_r])[0][0]\n",
    "        # print(f\"Similarity between {skill_j} and {skill_r}: {similarity}\")\n",
    "        # if the similarity score is above 0.7, remove skills in the dictionary mata_embedding_j\n",
    "        if similarity > 0.7 and skill_j in meta_embedding_j_copy.keys():\n",
    "            # print(f\"Similarity between {skill_j} and {skill_r}: {similarity}\")\n",
    "            # missing_skills.append(skill_j)\n",
    "            del meta_embedding_j_copy[skill_j]\n",
    "\n",
    "# Now similarities is a dictionary with skill pairs as keys and their cosine similarity as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data scraping', 'ai', 'data', 'software', 'database querying']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(meta_embedding_j_copy.keys())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data scraping, ai, data, software, database querying'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
